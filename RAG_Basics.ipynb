{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff40de5-549a-4205-bfc8-0731989f3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import torch\n",
    "import json\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e7973-f1d8-4864-af93-1b94c69c18a3",
   "metadata": {},
   "source": [
    "# RAG\n",
    "**Step1: Create A vector Database.**\n",
    "    - Parse the Content\n",
    "    - Create Chunks\n",
    "    - Create Embeddings\n",
    "**Step2: Create A Retrevel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c72fc-f244-4d4f-bfdf-8fdc051d10d4",
   "metadata": {},
   "source": [
    "# -----------------------------STEP 1 --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8298d38-6b8e-472f-bf22-e6dc435a0558",
   "metadata": {},
   "source": [
    "### ResumeParser\n",
    "- Create a **Function to Parse** all Resumes and return will be a dictionary. Key's of dictionary are Base path and Value's are the content of Resume.\n",
    "    - Input: Directory\n",
    "    - Define the directory of all Resumes papers.\n",
    "    - Read all Resumes available in the Directory and store their Content in Variable name CV\n",
    "    - Return: Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf3bdf9-fbbc-4339-a8b9-1300d2409b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResumeParser(Directory):\n",
    "    ResumeDict = {}\n",
    "    for Candidate in os.listdir(Directory):\n",
    "        PaperPath = os.path.join(Directory,Candidate)\n",
    "        base = os.path.basename(PaperPath)\n",
    "        if os.path.isfile(PaperPath):\n",
    "            with open(PaperPath,'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                TotalPages = len(reader.pages)\n",
    "                CV = \"\"\n",
    "                for page_num in range(TotalPages):\n",
    "                    page = reader.pages[page_num]\n",
    "                    CV += page.extract_text()\n",
    "            ResumeDict[base] = CV.strip()\n",
    "    return ResumeDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf77ef-a270-4294-b613-16aab7280bd6",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "- Create a Function to create chunks using the Content of all resumes.\n",
    "    - Create paragraphs using Desired Methods.\n",
    "    - Create Words using seprator = \" \"\n",
    "    - Create a list of chunks\n",
    "    - Generate unique Id for each item of chunk list.\n",
    "    - return all chunk dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6b9b05-2f6c-4b74-9832-9189b7e3d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGeneration(Content,para_seperator, ChunkSize,tokenizer,sku, separator=\" \"):\n",
    "    all_chunks = {}\n",
    "    paragraphs = re.split(para_seperator, Content)\n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph.split(separator)\n",
    "        current_chunk_str = \"\"\n",
    "        chunk = []\n",
    "        for word in words:\n",
    "            new_chunk = current_chunk_str + separator + word\n",
    "            new_chunk = new_chunk.strip()\n",
    "            new_chunk_tokens = tokenizer.tokenize(new_chunk)\n",
    "            if len(new_chunk_tokens) <= ChunkSize:\n",
    "                current_chunk_str = new_chunk\n",
    "            else:\n",
    "                chunk.append(current_chunk_str.strip())\n",
    "                current_chunk_str = \"\"\n",
    "        if len(current_chunk_str.strip())>0:\n",
    "            chunk.append(current_chunk_str.strip())\n",
    "        for item in chunk:\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            all_chunks[chunk_id] = {\"text\": item, \"metadata\": {\"file_name\":sku}}\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b8baa-03f7-469d-9952-b915ba420ba3",
   "metadata": {},
   "source": [
    "### ChunkPipeline\n",
    "- Create a Function to Manage the Workflow.\n",
    "    - Iterate over the Dictionary.\n",
    "    - Create a SKU (Stock Keeping Unit) using Key (base) of item.\n",
    "    - Generate a Unique ID for the each content using UUID.\n",
    "    - return a new Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58d0e32-332c-4cb1-a057-2cbe53fbafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkPipeline(Directory,ChunkSize,tokenizer):\n",
    "    documents = {}\n",
    "    ResumeDataBase = ResumeParser(Directory)\n",
    "    for item in ResumeDataBase:\n",
    "        sku = os.path.splitext(item)[0]\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        ChunkData = ChunkGeneration(ResumeDataBase[item],\"/n/n\", ChunkSize,tokenizer,sku)\n",
    "        documents[doc_id] = ChunkData\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92a133-71ef-4268-8970-7de260738d99",
   "metadata": {},
   "source": [
    "## VectorDataBase\n",
    "we need to obtain embeddings for each chunkâ€™s text and map these embeddings to their corresponding chunk IDs and document IDs.\n",
    "- Create a IndexerFunction which will iterate the DocID & Content of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886be4ed-52b2-468f-a32d-2bd758bbdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectoraization(Chunks,tokenizer,model):\n",
    "    MappedEmbeddings = {}\n",
    "    for DocId, DocContent in Chunks.items():\n",
    "        ChunkEmbeddings = {}\n",
    "        for ChunkID,ChunkContent in DocContent.items():\n",
    "            Content = ChunkContent[\"text\"]\n",
    "            inputs = tokenizer(Content, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            ChunkEmbeddings[ChunkID] = embeddings\n",
    "        MappedEmbeddings[DocId] = ChunkEmbeddings\n",
    "    return MappedEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90e53e-3484-491f-94b8-a24852c5a7f5",
   "metadata": {},
   "source": [
    "# -----------------------------STEP 2 --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5548688-c890-4681-8421-7e955dd184e5",
   "metadata": {},
   "source": [
    "Retrieval: For retrieving the answer to the query we will be using Cosine Similarity.\n",
    "- Create a function\n",
    "- Input: VectorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465ad963-72a4-4689-b26f-fe4ec2aaa86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Retrieval(query, top_k,VectorDataBase,tokenizer,model):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    query_embeddings=np.array(query_embeddings)\n",
    "    scores = {}\n",
    "    for doc_id, chunk_dict in VectorDataBase.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            chunk_embeddings = np.array(chunk_embeddings) \n",
    "            normalized_query = np.linalg.norm(query_embeddings)\n",
    "            normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "            if normalized_chunk == 0 or normalized_query == 0:\n",
    "                score == 0\n",
    "            else:\n",
    "                score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)  \n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535ec7e8-293e-449e-8177-9bcfc3476566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContextRetrieval(VectorDataBase,ContentTuple):\n",
    "    Context = \"\"\n",
    "    for item in ContentTuple:\n",
    "        docId = item[0]\n",
    "        ChunkId = item[1]\n",
    "        Context = Context + \"\\n\"+VectorDataBase[docId][ChunkId]['text']\n",
    "    return Context.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6b0e5-556e-427b-b0a5-e0079a940256",
   "metadata": {},
   "source": [
    "# -----------------------------STEP 3 --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20750dc-9b19-44de-996a-f75b1a711569",
   "metadata": {},
   "source": [
    "- Output Generation Using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb7a77f-2ced-406f-b175-2fb1186984a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_response(model,tokenizer,query, relavent_text):\n",
    "    template = \"\"\"\n",
    "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
    "\n",
    "    Your job is to understand the request, and answer based on the retrieved context.\n",
    "    Here is context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    template = template.format(context = relavent_text, question = query)\n",
    "    inputs = tokenizer(template, return_tensors=\"pt\")\n",
    "    # Generate text based on the input prompt\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=1000,  num_return_sequences=1,  no_repeat_ngram_size=2,  do_sample=True,  temperature=0.7 )\n",
    "    # Decode the generated tokens back into text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8247053-b7d6-4f6c-a7df-62f412a52ba8",
   "metadata": {},
   "source": [
    "## ModelLoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e78492-bd81-4086-812d-e6475ef09167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f151472-619b-4615-803c-29d557502b69",
   "metadata": {},
   "source": [
    "## Inputs for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1128e0e7-d5df-45e5-aa2e-bdf426fd3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Directory = r\"D:\\Coding\\RAG\\Documents\"\n",
    "ChunkSize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f42a71a-92b5-4a06-833c-5ca73519c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Passionate and results-oriented Generative AI Engineer with 2+ years of experience specializing in Natural Language Processing\n",
    " (NLP), Data Science, and Computer Vision. Leveraging a strong mathematical background and proven ability to develop\n",
    " innovative solutions, I contribute to cutting-edge projects at Collins Aerospaceâ€™s R&D team.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9506918-15d3-429d-9891-c79211a4b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chunks = ChunkPipeline(Directory,ChunkSize,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8482e126-8164-4f72-8193-6aba6ba2b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorDataBase = Vectoraization(Chunks,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c484e4e-6e1f-471d-a7c5-f1ee5149d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ContentDoc = Retrieval(query, 2,VectorDataBase,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351fbf1c-085d-4a82-9120-0c5e091e3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Context = ContextRetrieval(Chunks,ContentDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded9144c-cbb2-46eb-9e30-1059721308e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amank\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9172cb-08bb-4fd5-937a-2f47ac069023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\\n\\n    Your job is to understand the request, and answer based on the retrieved context.\\n    Here is context:\\n\\n    <context>\\n    projects at Collins Aerospaceâ€™s R&D team.\\nEducation\\nâ€¢Masters of Science in Mathematics and Computer Science from National Institute of Technology, Warangal with 7.1 GPA.\\nâ€¢Bachelor of Science in Mathematics from University of Delhi with 7.767 GPA.\\nExperience\\nGenerative AI Engineer, Raytheon Technology - Collins Aerospace (R&D) â€“ Hyd. July 2022 â€“ Present\\nâ€¢Applied prompt engineering on a LLM model to address 20% to 25% gaps caused\\nLinux\\nEngineering Skills: Generative-AI, NLP, AI-ML, Data Science, Computer Vision, Automation, DSA, OOPs, AR-VR\\nSoft Skills: Collaborator, Learner, Team Player, Innovator, Networking, Multi-Tasking\\nCertifications\\nâ€¢Language Processing Specialization, NLP with Python for Machine Learning, Prompt Engineering for ChatGPT.\\nâ€¢Generation Computing, ML & DL Applications, ML for Data Science using Python, Machine Learning & Cryptography.\\n    </context>\\n\\n    Question: Passionate and results-oriented Generative AI Engineer with 2+ years of experience specializing in Natural Language Processing\\n (NLP), Data Science, and Computer Vision. Leveraging a strong mathematical background and proven ability to develop\\n innovative solutions, I contribute to cutting-edge projects at Collins Aerospaceâ€™s R&D team.\\n     I have been recruited by an external company (Aero-Apparel, etc.) to work on various projects, including a C&C project for an existing company. \\xa0After a few months this project has been completed and I am looking for a contract. I did not choose this job. What is it like in a world with 100% artificial intelligence and artificial learning?\\nI work for the same company as my employer. We work together on our projects. In the beginning, the main focus is the computer vision project. The main task is processing data. During the evaluation phase, a team of four or five people, who work in different locations, work to solve problems. Then we work hard to find solutions. Once all the solutions are identified, we run the project, or work side by side, to make sure they are right. If there are any discrepancies, they will also be resolved. This is how it is. So, in the technical department, there is a lot of problem solving in front of us, like making a decision. When we start a project we don\\'t have time to think of what we can do to correct the problem. It is more like doing a series of tests and problems to get the right answer. As a result, you need to have the ability and skills to run a full-fledged project in your spare time. Since we are doing an automated project and only get results from the test, it should take only a minute or two to complete. However, if we decide to do a small project on an app, when we have all our problems solved properly, then after 10 to 15 seconds we get a perfect solution. That is why we decided to hire an artificial Intelligence Engineer. At the moment, he is only in his 20s and his job does not require any training. He also has a PhD in Computer Sciences and was a post-doc in Machine Science. Now, that we consider the role of an AI engineer, how will the position improve for him? How will he adapt to changing job, job type, company, industry and career?\\n\\n\\nIn the following interview, ask him the question \"What is your passion for AI in general and Machine-Learning in particular?\". In your next interview you will answer the questions in detail. Also, here is an overview of the interview. Please note that this interview was conducted using a Skype interview with a real-time AI expert, which was not possible for this person.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_llm_response(model,tokenizer,query, Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90023f4d-136c-4afd-87ae-b883df7376c4",
   "metadata": {},
   "source": [
    "## Important Steps of RAG\n",
    "- Data Chunks using chunking strategies\n",
    "- Data indexing in Vector Database using Indexing\n",
    "- ReRanking based on querry using cross encoders.\n",
    "- Generation using Retreived context and Querry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bb29bd-1a1c-4432-950e-59c4c553aa5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mImage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAG.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Show the image\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Image'"
     ]
    }
   ],
   "source": [
    "import Image\n",
    "img = Image.open('RAG.png')\n",
    "\n",
    "# Show the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a19db1-6832-462e-aa89-d9ef2134bbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
